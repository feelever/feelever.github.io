# Flink
### flink structure
#### 基础概念
![backpressure.png](/img/flink-struct.png)
* client-job->JobManager
* JobManager-coordinate checkpoint->job-task->TaskManager
* TaskManager-init slot->TaskManager-task handle->...
* StreamGraph(拓扑节点计划图):dataSource(1)->flatMap(2)->keyedBy(2)->dataSink(2)
* JobGraph(优化streamgraph而来):Socket Stream(1)->flatMap(2)->keyed agg && sinked(2)
* ExecutionGraph(并行化jobGraph而来)
* 物理执行图：根据ExecutionGraph部署各个task的真实执行
#### graph
![](/img/flink-graph-opr.png)
* StreamGraph
    * StreamNode 用来代表 operator 的类，并具有所有相关的属性，如并发度、入边和出边等
    * StreamEdge：表示连接两个StreamNode的边。
* JobGraph StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构
    * JobVertex：优化后多个StreamNode连接在一起生成一个JobVertex，即一个JobVertex包含一个或多个operator，JobVertex的输入是JobEdge，输出是IntermediateDataSet
    * IntermediateDataSet：JobVertex的输出，operator处理产生的数据集。consumer是JobEdge
    * 代表了job graph中的一条数据传输通道。target 是 JobVertex
* ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构，对应设计类为并行版job子类
* 物理执行图 JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构
    * Task：Execution被调度后在分配的 TaskManager 中启动对应的 Task。Task 包裹了具有用户执行逻辑的 operator
    * ResultPartition：代表由一个Task的生成的数据，和ExecutionGraph中的IntermediateResultPartition一一对应
    * ResultSubpartition：是ResultPartition的一个子分区。每个ResultPartition包含多个ResultSubpartition，其数目要由下游消费 Task 数和 DistributionPattern 来决定
    * InputGate：代表Task的输入封装，和JobGraph中JobEdge一一对应。每个InputGate消费了一个或多个的ResultPartition
    * InputChannel：每个InputGate会包含一个以上的InputChannel，和ExecutionGraph中的ExecutionEdge一一对应，也和ResultSubpartition一对一地相连，即一个InputChannel接收一个ResultSubpartition的输出
* 图设计目的：解耦，比如在用户提交这一层上解决批处理和流处理，批处理用OptimizedPlan，流处理用StreamGraph
![](/img/flink-graph-layer.png)
#### StreamGraph
* Transformation DataStream的底层其实就是一个 StreamTransformation ，创建完毕之后向env注册，env直接调用exec执行
![](/img/flink-streamgraph-trans1.png)
* StreamOperator是运行时Transformation的具体实现
#### DataStream
![](/img/flink-datastream-trans.png)
#### Window
Count Window
Time Window:Tumbling Window,Sliding Window,Session Window
#### table api
![](/img/flink-sql.png)
#### AsyncIo
![](/img/flink-asyncio.png)
### flink memory manage
#### 背景
1. java对象密度低
2. Full Gc影响非常大
3. OOM
#### 概述
* 内存分配单位MemorySegment(32KB) 类似 bytebuffer
![backpressure.png](/img/flink-memory-struct.jpg)
* Network Buffers 32KB大小的 buffer,用于数据的网络传输
* Memory Manager Pool:由 MemoryManager 管理的，由众多MemorySegment组成的超大集合
* Remaining (Free) Heap:主要都是由用户代码生成的短期对象
#### 二进制优化
Flink 通过 Java Reflection 框架分析基于 Java 的 Flink 程序 UDF (User Define Function)的返回类型的类型信息，通过 Scala Compiler 分析基于 Scala 的 Flink 程序 UDF 的返回类型的类型信息。类型信息由 TypeInformation 类表示,同时会生成TypeComparator用来进行compare,hash
```
BasicTypeInfo: 任意Java 基本类型（装箱的）或 String 类型。
BasicArrayTypeInfo: 任意Java基本类型数组（装箱的）或 String 数组。
WritableTypeInfo: 任意 Hadoop Writable 接口的实现类。
TupleTypeInfo: 任意的 Flink Tuple 类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现;
CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples)。
PojoTypeInfo: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter/setter 方法。
GenericTypeInfo: 任意无法匹配之前几种类型的类。
```
![](/img/flink-mmu-serializer.png)
flink中的operators采用类似 DBMS 的 sort 和 join 算法，直接操作二进制数据
![](/img/flink-mmu-byte-opr.png)
* 思路将数据分为对象自身二进制数据和定长的序列化key+pointer;
* sort以key为数据比对方法进行排序，并且key与key之间紧密排列
* 如果需要非key排序，就需要反序列化了。
* 同时，因为key+pointer的大小能比较轻松的装载到cache line 比从 memory中获取速度更快
![](/img/flink-mmu-byte-struct.png)
#### 堆外内存优化
利用堆外内存的zero-copy特性，进程间共享特性来提升性能，并且较少堆内存过大带来的启动以及GC弊端
* MemorySegment->HeapMemorySegment(短生命周期)/HybridMemorySegment（可堆内可堆外）
* factory 类会通过设定只提供其中一个MS，同时有大量final标识，用来触发JIT
### flink backpressure
流处理系统都会遇到反压问题，实现方案各有不通
* storm 监控Bolt中队列负载情况，超过一定水位通知zookeeper，利用zookeeper的消息通知Worker进入反压，spout停止发送tunple
* spark：服务注册rateController，监听到批次结束事件后采样计算新的消费速率， 提交job时利用消费速率计算每个分区消费的数据条数
#### flink 的方案
  ![backpressure.png](/img/flink-backpressure.png)
1. 基本概念
  * flink runtime 由operators 和 streams组成
  * LocalBufferPool用来分配可重复利用的buffer缓冲
2. 初始化
  * InputGate(IG)->Task->ResultPartition（RS）
  * TaskManager->NetworkEnvironment->NetworkBufferPool(2048)
  * NetworkEnvironment->LocalBufferPool->ResultPartition(subPartionSize)
3. 调用
  * Netty-data->Task-InputChannel->LocalBufferPool-无->NetworkBufferPool—无->stop to back pressure
4. 归还
  * Task-归还->LocalBufferPool-超限->NetworkBufferPool
#### 场景
* 本地传输，Task之间在同一TM，利用共享buffer，下游Task将buffer耗尽上游无法获取buffer来做反压
* 远程传输，下游Task无buffer可用，无法消费tcp连接中数据，上游通过netty的水位值机制实现停止下发，这时候该节点buffer也堆积起来，持续反压上游
* netty 水位值机制
  * 初始化时候设定水位大小，如果超过高水位，channel.isWritable=false
    ```java
    // 默认高水位值为2个buffer大小, 当接收端消费速度跟不上，发送端会立即感知到
    bootstrap.childOption(ChannelOption.WRITE_BUFFER_LOW_WATER_MARK, config.getMemorySegmentSize() + 1);
    bootstrap.childOption(ChannelOption.WRITE_BUFFER_HIGH_WATER_MARK, 2 * config.getMemorySegmentSize());
    ```
  * PartitionRequestQueue.writeAndFlushNextMessageIfPossible 做判断；通过channelWritabilityChanged对变更事件做处理
* 监控
  ![''](/img/flink-backpressure-moniter.png)
  * JobManager-akka->TaskManager::TriggerStackTraceSample->100 times/50ms stack trace->TaskManager::UI(60s resfresh)
### Flink 中的计算资源
#### Operator Chains
chain合并条件:
1. 上下游并行度一致；
2. 下游入度为1；
3. 上下游在同一个slot group
4. 下游节点chain策略为ALWAYS(map、flatmap、filter等默认是ALWAYS)
5. 上游节点的 chain 策略为 ALWAYS 或 HEAD(Source默认是HEAD)
6. 两个节点间数据分区方式是 forward
7. 用户没有禁用 chain
#### Task Slot
1. task slot 代表了 TaskManager 的一个固定大小的资源子集
2. slot目前仅仅用来隔离task的内存
3. 每个 slot 都能跑由多个连续 task 组成的一个 pipeline
4. 同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输。也能共享一些数据结构，一定程度上减少了每个task的消耗
![](/img/flink-slot-struct.png)
#### SlotSharingGroup 与 CoLocationGroup
Flink 允许subtasks共享slot，条件是它们都来自同一个Job的不同task的subtask
CoLocationGroup主要用于迭代流中，用来保证迭代头与迭代尾的第i个subtask能被调度到同一个TaskManager上
slot共享好处
1. task slots数与job中最高的并行度一致，不需要计算程序起多少task
2. 更容易获得更充分的资源利用
![](/img/flink-slot-sharing.png)
#### 估算方式
1. 记录数和每条记录的大小
2. 不同 key 的数量和每个 key 存储的 state 大小
3. 状态的更新频率和状态后端的访问模式
4. 网络容量
5. 磁盘带宽
6. 机器数量及其可用 CPU 和内存
### 高可用
1. 明确定义 Flink 算子的最大并发度
2. 为 Flink 算子指定唯一用户ID（UUID）
3. 充分考虑 Flink 程序的状态后端
4. 配置 JobManager 的高可用性（HA）
### 
以上内容为[http://wuchong.me](http://wuchong.me)中flink的阅读和个人摘要，直接访问t他的博客再结合版本更好