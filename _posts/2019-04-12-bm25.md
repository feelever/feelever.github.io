  
# bm25算法-文本相似度
  
## 原理
  
目的：用作搜索词相关性评分
  
原理步骤
 * 查询语句<img src="https://latex.codecogs.com/gif.latex?Q"/>语素解析得若干个<img src="https://latex.codecogs.com/gif.latex?q_i"/>
 * 对<img src="https://latex.codecogs.com/gif.latex?q_i"/>的搜索结果<img src="https://latex.codecogs.com/gif.latex?d"/>评分
 * 将<img src="https://latex.codecogs.com/gif.latex?q_i"/>与<img src="https://latex.codecogs.com/gif.latex?d"/> 加权<img src="https://latex.codecogs.com/gif.latex?W_i"/>评分累加
  
### 总公式1
  
 <p align="center"><img src="https://latex.codecogs.com/gif.latex?Score(Q,d)=&#x5C;sum_{i}^{n}W_i&#x5C;cdot%20R(q_i,d)"/></p>  
  
### <img src="https://latex.codecogs.com/gif.latex?W_i"/>说明
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?IDF(q_i)=log&#x5C;frac{N-n(q_i)+0.5}{n(q_i)+0.5}"/></p>  
  
* 公式比较常使用<img src="https://latex.codecogs.com/gif.latex?IDF"/>方式，如上
* <img src="https://latex.codecogs.com/gif.latex?N"/>:代表全部索引全部文档数
* <img src="https://latex.codecogs.com/gif.latex?n(q_i)"/>:包含<img src="https://latex.codecogs.com/gif.latex?q_i"/>的文档数
  
### <img src="https://latex.codecogs.com/gif.latex?R(q_i,d)"/>说明
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?R(q_i,d)=&#x5C;frac{f_i&#x5C;cdot%20(k_1+1)}{f_i+K}%20&#x5C;cdot%20&#x5C;frac{qf_i%20&#x5C;cdot(k_2+1)}{qf_i+k_2}"/></p>  
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?K=k_1%20&#x5C;cdot%20(1-b+b&#x5C;cdot&#x5C;frac{dl}{avgdl})"/></p>  
  
* <img src="https://latex.codecogs.com/gif.latex?k_1,k_2,b"/>为调节因子,通常会<img src="https://latex.codecogs.com/gif.latex?k_1=2,b=0.75"/>
* <img src="https://latex.codecogs.com/gif.latex?f_i"/>为<img src="https://latex.codecogs.com/gif.latex?q_i"/>在d中出现的频率，<img src="https://latex.codecogs.com/gif.latex?qf_i"/>为在Q中出现的频率，大部分情况下<img src="https://latex.codecogs.com/gif.latex?qf_i"/>为1
* <img src="https://latex.codecogs.com/gif.latex?dl"/>为文档d长度,<img src="https://latex.codecogs.com/gif.latex?avgdl"/>为平均文档长度
<p align="center"><img src="https://latex.codecogs.com/gif.latex?R(q_i,d)=&#x5C;frac{f_i%20&#x5C;cdot%20(k_i+1)}{f_i+K}"/></p>  
  
  
### 总公式2
  
 <p align="center"><img src="https://latex.codecogs.com/gif.latex?Score(Q,d)=&#x5C;sum_{i}^{n}IDF(q_i)&#x5C;cdot%20&#x5C;frac{f_i%20&#x5C;cdot%20(k_i+1)}{f_i+k_1%20&#x5C;cdot%20(1-b+b&#x5C;cdot&#x5C;frac{dl}{avgdl})}"/></p>  
  
  
####调试代码
  
```
class BM25(object):
 
    def __init__(self, docs):
        self.D = len(docs)
        self.avgdl = sum([len(doc)+0.0 for doc in docs]) / self.D
        self.docs = docs
        self.f = []  # 列表的每一个元素是一个dict，dict存储着一个文档中每个词的出现次数
        self.df = {} # 存储每个词及出现了该词的文档数量
        self.idf = {} # 存储每个词的idf值
        self.k1 = 1.5   #参数1，默认值是1.2  #####调参就是调这里的啦
        self.b = 0.75   #参数1，默认值为0.75
    
        self.init()
 
    def init(self):
        for doc in self.docs:
            tmp = {}
            for word in doc:
                tmp[word] = tmp.get(word, 0) + 1  # 存储每个文档中每个词的出现次数
            self.f.append(tmp)
            for k in tmp.keys():
                self.df[k] = self.df.get(k, 0) + 1        #分母+1，平滑处理，避免出现log(0)
        for k, v in self.df.items():
            self.idf[k] = math.log(self.D-v+0.5)-math.log(v+0.5)       #IDF的计算
 
    def sim(self, doc, index):
        score = 0
        for word in doc:
            if word not in self.f[index]:
                continue
            d = len(self.docs[index])  #k1
            score += (self.idf[word]*self.f[index][word]*(self.k1+1)
                      / (self.f[index][word]+self.k1*(1-self.b+self.b*d
                                                      / self.avgdl)))
        return score
 
    def simall(self, doc):
        scores = []
        for index in range(self.D):
            score = self.sim(doc, index)
            scores.append(score)
        return scores
```

  